{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_10.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw6zwoncX3ge"
      },
      "source": [
        "# 1.0 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVd2S1F5ZysZ"
      },
      "source": [
        "Our previous lesson discussed the GoogLeNet architecture and the Inception module, a micro-architecture that acts as a building block in the overall macro-architecture. We are now going to discuss another network architecture that relies on micro-architectures – **ResNet**.\n",
        "\n",
        "**ResNet** uses what is called a **residual module** to train Convolutional Neural Networks to depths previously thought impossible. For example, in 2014, the [VGG16 and VGG19](https://arxiv.org/abs/1409.1556) architectures were\n",
        "considered very deep. However, with [ResNet](https://arxiv.org/abs/1512.03385), we have successfully trained networks with > 100 layers on the challenging ImageNet dataset and over 1,000 layers on CIFAR-10.\n",
        "\n",
        "These depths are only made possible by using *smarter* weight initialization algorithms (such as [Xavier/Glorot](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [MSRA/He](https://arxiv.org/abs/1502.01852) et al.) along with **identity mapping**, a concept we’ll discuss later in this lesson. Given the depths of ResNet networks, perhaps it comes as no surprise that ResNet took first place in all three ILSVRC 2015 challenges (classification, detection, and\n",
        "localization).\n",
        "\n",
        "In this lesson, we are going to discuss the ResNet architecture, the residual module, along with updates to the residual module that has made it capable of obtaining higher classification accuracy.\n",
        "From there, we’ll implement and train variants of ResNet on the CIFAR-10 dataset and the Tiny ImageNet challenge – in each case, our ResNet implementations will outperform every experiment we have executed in this course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuFRMY6Ma2mk"
      },
      "source": [
        "# 2.0 ResNet and the Residual Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRUa3t5WcgtX"
      },
      "source": [
        "First introduced by He et al. in their 2015 paper, [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), the **ResNet** architecture has become a seminal work, demonstrating that extremely deep networks can be trained using standard SGD and a reasonable initialization function. In order to train networks at depths greater than 50-100 (and in some cases, 1,000) layers, ResNet relies on a micro-architecture called the **residual module**.\n",
        "\n",
        "Another interesting component of ResNet is that pooling layers are used extremely sparingly.\n",
        "\n",
        "Building on the work of [Springenberg](https://arxiv.org/abs/1412.6806) et al., ResNet does not strictly rely on max pooling operations to reduce volume size. Instead, convolutions with strides > 1 are used to learn weights and reduce the output volume spatial dimensions. In fact, there are only two occurrences of pooling being applied in the full implementation of the architecture:\n",
        "\n",
        "1. The first (and only) occurrence of max pooling happens early in the network to help reduce spatial dimensions.\n",
        "\n",
        "2. The second pooling operation is an average pooling layer used in place of fully connected layers, like in GoogLeNet.\n",
        "\n",
        "Strictly speaking, there is only one max pooling layer – convolutional layers handle all other reductions in spatial dimensions.\n",
        "\n",
        "In this section, we’ll review the original residual module, along with the residual bottleneck module used to train deeper networks. From there, we’ll discuss extensions and updates to the original residual module by He et al. in their 2016 publication, [Identity Mappings in Deep\n",
        "Residual Networks](https://arxiv.org/abs/1603.05027), that allow us to further increase classification accuracy. Later in this lesson, we’ll implement ResNet from scratch using Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuZ3IlpQdJ3b"
      },
      "source": [
        "## 2.1 Going Deeper: Residual Modules and Bottlenecks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuVUm8a2e0WK"
      },
      "source": [
        "The original residual module introduced by [He et al.](https://arxiv.org/abs/1512.03385) in 2015 relies on **identity mappings**, \n",
        "\n",
        "> the process of taking the original input to the module and adding it to the output of a series of operations. \n",
        "\n",
        "A graphical depiction of this module can be seen in Figure below (left). Notice how this module only has two branches, unlike the four branches in the Inception module of GoogLeNet. Furthermore,\n",
        "this module is highly simplistic.\n",
        "\n",
        "\n",
        "<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1Hw_1H_Bm9oPuLPRH12yy3r4eYPJSaXEu\"/>\n",
        "\n",
        "At the top of the module, we accept input to the module (i.e., the previous layer in the network). The right branch is a **linear shortcut** – it connects the input to an addition operation at the bottom of the module. Then, on the left branch of the **residual module**, we apply a series of\n",
        "convolutions (all of which are 3 x 3), activations, and batch normalizations. This is a fairly standard pattern to follow when constructing Convolutional Neural Networks.\n",
        "\n",
        "But what makes ResNet interesting is that He et al. suggested adding the original input to the output of the CONV, RELU and BN layers. We call this addition an **identity mapping** since the input\n",
        "(the identity) is added to the output of series of operations. It is also why the term **residual** is used. The **residual** input is added to the output of a series of layer operations. The connection\n",
        "between the input and the addition node is called the **shortcut**. Note that we are not referring to\n",
        "concatenation along the channel dimension as we have done in previous lessons. Instead, we are performing simple 1+1 = 2 addition at the bottom of the module between the two branches.\n",
        "\n",
        "While traditional neural network layers can be seen as learning a function $y = f(x)$, a residual layer attempts to approximate $y$ via $f(x)+id(x) = f (x)+x$ where $id(x)$ is the identity function.\n",
        "\n",
        "These residual layers start at the identity function and evolve to become more complex as the network learns. This type of residual learning framework allows us to train networks that are substantially\n",
        "deeper than previously proposed network architectures.\n",
        "\n",
        "Furthermore, since the input is included in every residual module, it turns out the network can learn faster and with larger learning rates. It is very common to see the base learning rates for\n",
        "ResNet implementations start at $1e-1$. For most architectures such as AlexNet or VGGNet, this high of a learning rate would almost guarantee the network would not converge. But since ResNet\n",
        "relies on residual modules via identity mappings, this higher learning rate is completely possible.\n",
        "\n",
        "In the same 2015 work, [He et al.](https://arxiv.org/abs/1512.03385) also included an extension to the original residual module called **bottlenecks** (Figure above, right). Here we can see that the same identity mapping is taking\n",
        "place, only now the CONV layers in the left branch of the residual module have been updated:\n",
        "\n",
        "1. We are utilizing three CONV layers rather than just two.\n",
        "2. The first and last CONV layers are 1\u0002x 1 convolutions.\n",
        "3. The number of filters learned in the first two CONV layers are 1/4 the number of filters learned\n",
        "in the final CONV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8HTI1YhfJuF"
      },
      "source": [
        "To understand why we call this a **bottleneck**, consider the following figure where two residual modules are stacked on top of each other, with one residual feeding into the next (Figure below).\n",
        "\n",
        "\n",
        "<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=11Ag3EgyXAf5fbgJd-YGO1y6ue3kxGRrj\"/>\n",
        "\n",
        "The first residual module accepts an input volume of size M\u0002x N x 64 (the actual width and height are arbitrary for this example). The three CONV layers in the first residual module learn\n",
        "K = 32, 32, and 128 filters, respectively. After applying the first residual module our output volume size is M x\u0002N x 128 which is then fed into the second residual module.\n",
        "\n",
        "In the second residual module, our number of filters learned by each of the three CONV layers stays the same at K = 32, 32, and 128, respectively. However, notice that 32 < 128, implying that\n",
        "we are actually reducing the volume size during the 1 x\u00021 and 3\u0002x 3 CONV layers. This result has the benefit of leaving the 3x3 bottleneck layer with smaller input and output dimensions.\n",
        "\n",
        "The final 1x1 CONV then applies 4x the number of filters than the first two CONV layers, thereby increasing dimensionality once again, which is why we call this update to the residual module\n",
        "the **bottleneck** technique. When building our own residual modules, it’s common to supply pseudocode such as residual_module(K=128) which implies that the final CONV layer will learn\n",
        "128 filters, while the first two will learn 128/4 = 32 filters. This notation is often easier to work with as it’s understood that the bottleneck CONV layers will learn 1/4th the number of filters as the\n",
        "final CONV layer.\n",
        "\n",
        "When it comes to training **ResNet**, we typically use the **bottleneck variant** of the residual module rather than the original version, especially for ResNet implementations with > 50 layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZoW4va9qjmh"
      },
      "source": [
        "## 2.2 Rethinking the Residual Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amTBAGp69M9u"
      },
      "source": [
        "In 2016, He et al. published a second paper on the residual module entitled [Identity Mappings\n",
        "in Deep Residual Networks](https://arxiv.org/abs/1603.05027). This publication described a comprehensive study, both theoretically and empirically, on the ordering of convolutional, activation, and batch normalization\n",
        "layers within the residual module itself. Originally, the residual module (with bottleneck) looked like Figure below (left).\n",
        "\n",
        "<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1Gl7Dzj3rKcSP5WW0RWvB4i3h2MroXljX\"/>\n",
        "\n",
        "The original residual module with bottleneck accepts an input (a ReLU activation map) and then applies a series of (CONV => BN => RELU) * 2 => CONV => BN before adding this output to the original input and applying a final ReLU activation (which is then fed into the next residual module in the network). However, the He et al. 2016 study, it was found there was a more optimal layer ordering capable of obtaining higher accuracy – this method is called **pre-activation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUchn63K9czU"
      },
      "source": [
        "In the **pre-activation** version of the residual module, we remove the ReLU at the bottom of the module and re-order the batch normalization and activation such that they come before the\n",
        "convolution (Figure above, right).\n",
        "\n",
        "Now, instead of starting with a convolution, we apply a series of (BN => RELU => CONV) * 3 (assuming the bottleneck is being used, of course). The output of the residual module is now the\n",
        "addition operation which is subsequently fed into the next residual module in the network (since residual modules are stacked on top of each other).\n",
        "\n",
        "We call this layer ordering pre-activation as our ReLUs and batch normalization are placed before the convolutions, which is in contrast to the typical approach of applying ReLUs and batch\n",
        "normalizations after the convolutions. In our next section, we’ll implement ResNet from scratch using both **bottlenecks** and **pre-activations**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ1KAAGe-nSw"
      },
      "source": [
        "# 3.0 Implementing ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXMe4Yge_bSk"
      },
      "source": [
        "Now that we have reviewed the ResNet architecture, let’s go ahead and implement in Keras. For\n",
        "this specific implementation, we’ll be using the most recent incarnation of the residual module,\n",
        "including bottlenecks and pre-activations.\n",
        "\n",
        "We start off by importing our fairly standard set of classes and functions when building Convolutional Neural Networks. However, I would like to draw your attention to **Line 12** where we\n",
        "import the add function. Inside the residual module, we’ll need to add together the outputs of two branches, which will be accomplished via this add method. We’ll also import the **l2 function** on\n",
        "**Line 13** so that we can perform **L2 weight decay**. \n",
        "> Regularization is extremely important when training ResNet since, due to the network’s depth, it is prone to overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ4bneGh8nKN"
      },
      "source": [
        "\n",
        "Next, let’s move on to our residual_module:\n",
        "\n",
        "```python\n",
        "class ResNet:\n",
        "\t@staticmethod\n",
        "\tdef residual_module(data, K, stride, chanDim, red=False, reg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
        "```\n",
        "\n",
        "This specific implementation of ResNet was inspired by both He et al. in their [Caffe distribution](https://github.com/KaimingHe/deep-residual-networks) as well as the mxnet implementation from [Wei Wu](https://github.com/tornadomeet/ResNet), therefore we will follow their parameter choices as closely as possible. Looking at the residual_module we can see that the function accepts more parameters than any of our previous functions – let’s review each of them in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlEO1-AX9mn6"
      },
      "source": [
        "The data parameter is simply the input to the residual module. The **value K** defines the number of filters that will be learned by the final CONV in the bottleneck. The first two CONV layers will\n",
        "learn K / 4 filters, as per the He et al. paper. \n",
        "\n",
        "**The stride** controls the stride of the convolution.\n",
        "We’ll use this parameter to help us reduce the spatial dimensions of our volume without resorting to max pooling.\n",
        "\n",
        "We then have the **chanDim parameter** which defines the axis which will perform batch normalization – this value is specified later in the build function based on whether we are using “channels\n",
        "last” or “channels first” ordering.\n",
        "\n",
        "**Not all residual modules will be responsible for reducing the dimensions of our spatial volume** – the red (i.e., “reduce”) boolean will control whether we are reducing spatial dimensions (True) or not (False)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA3ZSfNK-U3q"
      },
      "source": [
        "We can then supply a regularization strength to all CONV layers in the residual module **via reg**. The **bnEps parameter** controls the $\\epsilon$ responsible for avoiding “division by zero” errors when normalizing inputs. In Keras, $\\epsilon$ defaults to 0.001; however, for our particular implementation, we’ll allow this value to be reduced significantly. The **bnMom controls** the momentum for the moving average. This value normally defaults to 0.99 inside Keras, but He et al. as well as Wei Wu recommend decreasing the value to 0.9.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUqJkmac--3y"
      },
      "source": [
        "\n",
        "Now that the parameters of **residual_module** are defined, let’s move on to the body of the function:\n",
        "\n",
        "```python\n",
        "19:        # the shortcut branch of the ResNet module should be\n",
        "20:\t\t# initialize as the input (identity) data\n",
        "21:\t\tshortcut = data\n",
        "22:\n",
        "23:\t\t# the first block of the ResNet module are the 1x1 CONVs\n",
        "24:\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(data)\n",
        "25:\t\tact1 = Activation(\"relu\")(bn1)\n",
        "26:\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
        "````"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehd9ILNm_tq9"
      },
      "source": [
        "On **Line 21** we initialize the **shortcut** in the residual module, which is simply a reference to the **input data**. We will later add the shortcut to the output of our bottleneck + pre-activation branch.\n",
        "\n",
        "The first pre-activation of the bottleneck branch can be seen in Lines 24-26. Here we apply batch normalization layer, followed by ReLU activation, and then a 1x1 convolution, using K/4 total filters. **You’ll also notice that we are excluding the bias term from our CONV layers via use_bias=False**. Why might we wish to purposely leave out the bias term? [According to He et\n",
        "al., the biases are in the BN layers that immediately follow the convolutions](https://github.com/KaimingHe/deep-residual-networks/issues/10#issuecomment-194037195), so there is no need to introduce a second bias term."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwQinG3BAznp"
      },
      "source": [
        "Next, we have our second CONV layer in the bottleneck, this one responsible for learning a total of K/4, 3x3 filters according to **Lines 28 to 31**.\n",
        "\n",
        "The final block in the bottleneck learns K filters, each of which are 1x1 acording to **Lines 33 to 37**.\n",
        "\n",
        "The next step is to see if we need to reduce spatial dimensions, thereby alleviating the need to\n",
        "apply max pooling:\n",
        "\n",
        "```python\n",
        "39:    # if we are to reduce the spatial size, apply a CONV layer to\n",
        "40:    # the shortcut\n",
        "41:\t\tif red:\n",
        "42:\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride, use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
        "```\n",
        "\n",
        "If we are instructed to reduce spatial dimensions, we’ll do so with a convolutional layer (applied to the shortcut) with a stride > 1. The output of the final conv3 in the bottleneck is the added together with the shortcut, thus serving as the output of the **residual_module**:\n",
        "\n",
        "\n",
        "```python\n",
        "44:    # add together the shortcut and the final CONV\n",
        "45:    x = add([conv3, shortcut])\n",
        "46:\n",
        "47:    # return the addition as the output of the ResNet module\n",
        "48:    return x\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTQghrWfn8Ip"
      },
      "source": [
        "The **residual_module** will serve as our building block when creating deep residual networks.\n",
        "Let’s move on to using this building block inside the build method:\n",
        "\n",
        "```python\n",
        "@staticmethod\n",
        "\tdef build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdu-WvIvxmyH"
      },
      "source": [
        "Just as our **residual_module** requires more parameters than previous micro-architecture implementations, the same is true for our **build** function. The **width, height, and depth** classes\n",
        "all control the input spatial dimensions of the images in our dataset. The classes variable dictates how many overall classes our network should learn – these variables you have already seen.\n",
        "\n",
        "> What is interesting are the **stages** and **filters** parameters, both of which are **lists**. \n",
        "\n",
        "When constructing the **ResNet architecture**, we’ll be stacking a number of residual modules on top of each other (using the same number of filters for each stack), followed by reducing the spatial dimensions of the volume – this process is then continued until we are ready to apply our average pooling and softmax classifier.\n",
        "\n",
        "\n",
        "To make this point clear, let’s suppose that stages=(3, 4, 6) and filters=(64, 128, 256, 512). The first filter value, 64, will be applied to the only CONV layer not part of the residual module (i.e, first convolutional layer in the network). We’ll then stack three residual modules on top of each other – each of these residual modules will learn K = 128 filters. The spatial dimensions of the volume will be reduced, and then we’ll move on to the second entry in stages where we’ll stack four residual modules on top of each other, each responsible for learning K = 256 filters. After these four residual modules, we’ll again reduce dimensionality and move on to the final entry in the stages list, instructing us to stack six residual modules on top of each other, where each residual module will learn K = 512."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vlDm0fklVT3"
      },
      "source": [
        "The benefit of specifying both **stages** and **filters** in a list (rather than hardcoding them) is that we can easily leverage for loops to build the very deep network architectures without introducing code bloat – this point will become more clear later in our implementation. For the sake of understanding, in the [He at al., 2015](https://arxiv.org/pdf/1512.03385.pdf), Table 1, it is possible to note the different stages in ResNet. Considering the 152-layer model, stages will be (3,8,36,3) whereas filters are (64 \\<conv1\\>, 256, 512, 1024, 2048).\n",
        "\n",
        "<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1D1IzM7a2BnP1TkX0Nq21eNADpWY5HXyX\"/>\n",
        "\n",
        "Finally, we have the dataset parameter which is assumed to be a string. Depending on\n",
        "the dataset we are building ResNet for, we may want to apply more/less convolutions and batch normalizations before we start stacking our residual modules. We’ll see why we might want to vary the number of convolutional layers latter, but for the time being, you can safely ignore this parameter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjzhVvljnwFF"
      },
      "source": [
        "Unlike previous network architectures we have seen in this course (where the first layer is typically a CONV), we see that ResNet uses a BN as the first layer. \n",
        "\n",
        "> The reasoning behind applying batch normalization to your input is an added level of normalization. \n",
        "\n",
        "In fact, **performing batch normalization on the input itself can sometimes remove the need to apply mean normalization to the inputs**. In either case, the BN on **Line 68** acts as an added level of normalization.\n",
        "\n",
        "```python\n",
        "\t\t# set the input and apply BN\n",
        "\t\tinputs = Input(shape=inputShape)\n",
        "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(inputs)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny6iWT0_peIW"
      },
      "source": [
        "From there, we apply a single CONV layer on **Lines 70 to 73**. This CONV layer will learn a total of filters[0], 3x3 filters (keep in mind that filters is a list, so this value is specified via the build method when constructing the architecture).\n",
        "\n",
        "You’ll also notice that I’ve made a check to see if we are using the CIFAR-10 dataset (**Line 71**). Later in this lesson, we’ll explain the elif statement for Tiny ImageNet.\n",
        "Since the input dimensions to Tiny ImageNet are larger, we’ll apply a series of convolutions, batch normalizations, and max pooling (the only max pooling in the ResNet architecture) before we start stacking residual modules. However, for the time being, we are only using the CIFAR-10 dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWNDba4gqP62"
      },
      "source": [
        "Let’s go ahead and start stacking residual layers on top of each other, the cornerstone of the\n",
        "ResNet architecture:\n",
        "\n",
        "```python\n",
        "84:\t\t# loop over the number of stages\n",
        "85:\t\tfor i in range(0, len(stages)):\n",
        "86:\t\t\t# initialize the stride, then apply a residual module\n",
        "87:\t\t\t# used to reduce the spatial size of the input volume\n",
        "88:\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n",
        "89:\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride, chanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
        "90:\n",
        "91:\t\t\t# loop over the number of layers in the stage\n",
        "92:\t\t\tfor j in range(0, stages[i] - 1):\n",
        "93:\t\t\t\t# apply a ResNet module\n",
        "94:\t\t\t\tx = ResNet.residual_module(x, filters[i + 1], (1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
        "```\n",
        "\n",
        "On **Line 85** we start looping over the list of stages. Keep in mind that every entry in the **stages list** is an integer, indicating how many residual modules will be stacked on top of each other. Following the work of [Springenberg et al.](https://www.arxiv-vanity.com/papers/1412.6806/), ResNet tries to reduce the usage of pooling as\n",
        "much as possible, relying on CONV layers to reduce the spatial dimensions of a volume.\n",
        "\n",
        "To reduce volume size without pooling layers, we must set the stride of the convolution on **Line 88**. If this is the first entry in the stage, we’ll set the stride to (1, 1), indicating that no downsampling should be performed. However, for every subsequent stage we’ll apply a residual module with a stride of (2, 2), which will allow us to decrease the volume size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJXP8EWHwgUz"
      },
      "source": [
        "From there, we’ll loop over the number of layers in the current stage on **Line 92** (i.e., the number of residual modules that will be stacked on top of each other). The number of filters each residual module will learn is controlled by the corresponding entry in the filters list. The reason we use i + 1 as the index into filters is because the first filter value was used on **Line 73**. The rest of the filter values correspond to the number of filters in each stage. Once we have stacked stages[i] residual modules on top of each other, our for loop brings us back up to **Line 89** where we decrease the spatial dimensions of the volume and repeat the process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQMv_KThxZ9y"
      },
      "source": [
        "At this point, our volume size has been reduced to **8 x 8 x num_filters** (you can verify this for yourself by computing the input/output volume sizes for each layer. In order to avoid using dense fully-connected layers, we’ll instead apply average pooling to\n",
        "reduce the volume size to 1 x 1 x classes:\n",
        "\n",
        "```python\n",
        "\t\t# apply BN => ACT => POOL\n",
        "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,momentum=bnMom)(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = AveragePooling2D((8, 8))(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDKXKQ88xyii"
      },
      "source": [
        "From there, we create a dense layer for the total number of classes we are going to learn, followed by applying a softmax activation to obtain our final output probabilities:\n",
        "\n",
        "```python\n",
        "\t\t# softmax classifier\n",
        "\t\tx = Flatten()(x)\n",
        "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
        "\t\tx = Activation(\"softmax\")(x)\n",
        "\n",
        "\t\t# create the model\n",
        "\t\tmodel = Model(inputs, x, name=\"resnet\")\n",
        "\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwEBH5Pu_njn"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import ZeroPadding2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class ResNet:\n",
        "\t@staticmethod\n",
        "\tdef residual_module(data, K, stride, chanDim, red=False, reg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
        "    # \n",
        "    # based on pre-activation residual module\n",
        "    #\n",
        "\t\t# the shortcut branch of the ResNet module should be\n",
        "\t\t# initialize as the input (identity) data\n",
        "\t\tshortcut = data\n",
        "\n",
        "\t\t# the first block of the ResNet module are the 1x1 CONVs\n",
        "\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(data)\n",
        "\t\tact1 = Activation(\"relu\")(bn1)\n",
        "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
        "\n",
        "\t\t# the second block of the ResNet module are the 3x3 CONVs\n",
        "\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(conv1)\n",
        "\t\tact2 = Activation(\"relu\")(bn2)\n",
        "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride, padding=\"same\", use_bias=False, kernel_regularizer=l2(reg),)(act2)\n",
        "\n",
        "\t\t# the third block of the ResNet module is another set of 1x1\n",
        "\t\t# CONVs\n",
        "\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(conv2)\n",
        "\t\tact3 = Activation(\"relu\")(bn3)\n",
        "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,kernel_regularizer=l2(reg))(act3)\n",
        "\n",
        "\t\t# if we are to reduce the spatial size, apply a CONV layer to\n",
        "\t\t# the shortcut\n",
        "\t\tif red:\n",
        "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride, use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
        "\n",
        "\t\t# add together the shortcut and the final CONV\n",
        "\t\tx = add([conv3, shortcut])\n",
        "\n",
        "\t\t# return the addition as the output of the ResNet module\n",
        "\t\treturn x\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
        "\t\t# initialize the input shape to be \"channels last\" and the\n",
        "\t\t# channels dimension itself\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "\t\t# set the input and apply BN\n",
        "\t\tinputs = Input(shape=inputShape)\n",
        "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(inputs)\n",
        "\n",
        "\t\t# check if we are utilizing the CIFAR dataset\n",
        "\t\tif dataset == \"cifar\":\n",
        "\t\t\t# apply a single CONV layer\n",
        "\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False, padding=\"same\", kernel_regularizer=l2(reg))(x)\n",
        "\n",
        "\t\t# check to see if we are using the Tiny ImageNet dataset\n",
        "\t\telif dataset == \"tiny_imagenet\":\n",
        "\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\n",
        "\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False, padding=\"same\", kernel_regularizer=l2(reg))(x)\n",
        "\t\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(x)\n",
        "\t\t\tx = Activation(\"relu\")(x)\n",
        "\t\t\tx = ZeroPadding2D((1, 1))(x)\n",
        "\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "\t\t# loop over the number of stages\n",
        "\t\tfor i in range(0, len(stages)):\n",
        "\t\t\t# initialize the stride, then apply a residual module\n",
        "\t\t\t# used to reduce the spatial size of the input volume\n",
        "\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n",
        "\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride, chanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
        "\n",
        "\t\t\t# loop over the number of layers in the stage\n",
        "\t\t\tfor j in range(0, stages[i] - 1):\n",
        "\t\t\t\t# apply a ResNet module\n",
        "\t\t\t\tx = ResNet.residual_module(x, filters[i + 1], (1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
        "\n",
        "\t\t# apply BN => ACT => POOL\n",
        "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,momentum=bnMom)(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = AveragePooling2D((8, 8))(x)\n",
        "\n",
        "\t\t# softmax classifier\n",
        "\t\tx = Flatten()(x)\n",
        "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
        "\t\tx = Activation(\"softmax\")(x)\n",
        "\n",
        "\t\t# create the model\n",
        "\t\tmodel = Model(inputs, x, name=\"resnet\")\n",
        "\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpH2QNOzEY7V"
      },
      "source": [
        "model = ResNet.build(32, 32, 3, 10, (9, 9, 9),(64, 64, 128, 256), reg=0.0005)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eETjtZvJDmdI"
      },
      "source": [
        "# 4.0 ResNet on CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRshWpRC-J4g"
      },
      "source": [
        "Outside of training smaller variants of ResNet on the full ImageNet dataset, I had never attempted to train ResNet on CIFAR-10 (or Stanford’s Tiny ImageNet challenge, as we’ll see in this section). Because of this fact, I have decided to treat this section and the next as candid case studies where I reveal my personal rules of thumb and best practices I have mentioned in the previous lessons. These best practices allow me to approach a new problem with an initial plan, iterate on it, and eventually arrive at a solution that obtains good accuracy. In the case of CIFAR-10, we’ll be able to replicate the performance of **He et al.** and claim a spot amongst other [state-of-the-art approaches](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7I0Pi1rAxQ0"
      },
      "source": [
        "## 4.1 Useful functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhNuR7lsI4-s"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.callbacks import BaseLogger\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "class TrainingMonitor(BaseLogger):\n",
        "\tdef __init__(self, figPath, jsonPath=None, startAt=0):\n",
        "\t\t# store the output path for the figure, the path to the JSON\n",
        "\t\t# serialized file, and the starting epoch\n",
        "\t\tsuper(TrainingMonitor, self).__init__()\n",
        "\t\tself.figPath = figPath\n",
        "\t\tself.jsonPath = jsonPath\n",
        "\t\tself.startAt = startAt\n",
        "\n",
        "\tdef on_train_begin(self, logs={}):\n",
        "\t\t# initialize the history dictionary\n",
        "\t\tself.H = {}\n",
        "\n",
        "\t\t# if the JSON history path exists, load the training history\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tif os.path.exists(self.jsonPath):\n",
        "\t\t\t\tself.H = json.loads(open(self.jsonPath).read())\n",
        "\n",
        "\t\t\t\t# check to see if a starting epoch was supplied\n",
        "\t\t\t\tif self.startAt > 0:\n",
        "\t\t\t\t\t# loop over the entries in the history log and\n",
        "\t\t\t\t\t# trim any entries that are past the starting\n",
        "\t\t\t\t\t# epoch\n",
        "\t\t\t\t\tfor k in self.H.keys():\n",
        "\t\t\t\t\t\tself.H[k] = self.H[k][:self.startAt]\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# loop over the logs and update the loss, accuracy, etc.\n",
        "\t\t# for the entire training process\n",
        "\t\tfor (k, v) in logs.items():\n",
        "\t\t\tl = self.H.get(k, [])\n",
        "\t\t\tl.append(float(v))\n",
        "\t\t\tself.H[k] = l\n",
        "\n",
        "\t\t# check to see if the training history should be serialized\n",
        "\t\t# to file\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tf = open(self.jsonPath, \"w\")\n",
        "\t\t\tf.write(json.dumps(self.H))\n",
        "\t\t\tf.close()\n",
        "\n",
        "\t\t# ensure at least two epochs have passed before plotting\n",
        "\t\t# (epoch starts at zero)\n",
        "\t\tif len(self.H[\"loss\"]) > 1:\n",
        "\t\t\t# plot the training loss and accuracy\n",
        "\t\t\tN = np.arange(0, len(self.H[\"loss\"]))\n",
        "\t\t\tplt.style.use(\"ggplot\")\n",
        "\t\t\tplt.figure()\n",
        "\t\t\tplt.plot(N, self.H[\"loss\"], label=\"train_loss\")\n",
        "\t\t\tplt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\n",
        "\t\t\tplt.plot(N, self.H[\"accuracy\"], label=\"train_acc\")\n",
        "\t\t\tplt.plot(N, self.H[\"val_accuracy\"], label=\"val_acc\")\n",
        "\t\t\tplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(\n",
        "\t\t\t\tlen(self.H[\"loss\"])))\n",
        "\t\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\t\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\t\t\tplt.legend()\n",
        "\n",
        "\t\t\t# save the figure\n",
        "\t\t\tplt.savefig(self.figPath)\n",
        "\t\t\tplt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlPCafksJBLo"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import os\n",
        "\n",
        "class EpochCheckpoint(Callback):\n",
        "\tdef __init__(self, outputPath, every=5, startAt=0):\n",
        "\t\t# call the parent constructor\n",
        "\t\tsuper(Callback, self).__init__()\n",
        "\n",
        "\t\t# store the base output path for the model, the number of\n",
        "\t\t# epochs that must pass before the model is serialized to\n",
        "\t\t# disk and the current epoch value\n",
        "\t\tself.outputPath = outputPath\n",
        "\t\tself.every = every\n",
        "\t\tself.intEpoch = startAt\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# check to see if the model should be serialized to disk\n",
        "\t\tif (self.intEpoch + 1) % self.every == 0:\n",
        "\t\t\tp = os.path.sep.join([self.outputPath,\n",
        "\t\t\t\t\"epoch_{}.hdf5\".format(self.intEpoch + 1)])\n",
        "\t\t\tself.model.save(p, overwrite=True)\n",
        "\n",
        "\t\t# increment the internal epoch counter\n",
        "\t\tself.intEpoch += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GoFTNJ-ymJg"
      },
      "source": [
        "# create some folder to store results and metadata\n",
        "!mkdir output\n",
        "!mkdir output/checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpCGYsrYBmd5"
      },
      "source": [
        "#\n",
        "# import libraries, load dataset and pre-processing\n",
        "#\n",
        "\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# load the training and testing data, converting the images from\n",
        "# integers to floats\n",
        "print(\"[INFO] loading CIFAR-10 data...\")\n",
        "((train_x, train_y), (test_x, test_y)) = cifar10.load_data()\n",
        "train_x = train_x.astype(\"float\")\n",
        "test_x = test_x.astype(\"float\")\n",
        "\n",
        "# apply mean subtraction to the data\n",
        "mean = np.mean(train_x, axis=0)\n",
        "train_x -= mean\n",
        "test_x -= mean\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "train_y = lb.fit_transform(train_y)\n",
        "test_y = lb.transform(test_y)\n",
        "\n",
        "# construct the image generator for data augmentation\n",
        "aug = ImageDataGenerator(width_shift_range=0.1, \n",
        "                         height_shift_range=0.1, \n",
        "                         horizontal_flip=True,\n",
        "                         fill_mode=\"nearest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsbv9Jv8_J25"
      },
      "source": [
        "## 4.2 Training ResNet on CIFAR-10 With the ctrl + c Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yTUdAXt_kgr"
      },
      "source": [
        "Whenever I start a new set of experiments with either a network architecture I am unfamiliar with, a dataset I have never worked with, or both, I always begin with the ctrl + c method of training. \n",
        "\n",
        "Using this method:\n",
        "\n",
        "1. I can start training with an initial learning rate (and associated set of hyperparameters), \n",
        "2. monitor training, \n",
        "3. and quickly adjust the learning rate based on the results as they come in. \n",
        "\n",
        "This method is especially helpful when I am totally unsure on the approximate number of epochs it will take for a given architecture to obtain reasonable accuracy or a specific dataset.\n",
        "\n",
        "In the case of CIFAR-10, I have previous experience (as do you, after studying the previous lessons), so I’m quite confident that it will take 60-100 epochs, but I’m not exactly sure since I’ve never trained ResNet on the CIFAR-10 before.\n",
        "\n",
        "Therefore, our first few experiments will rely on the ctrl + c method of training to narrow in on what hyperparameters we should be using. Once we are comfortable with our set of hyperparameters, we’ll switch over to a specific learning rate decay schedule in hopes of milking every last bit of accuracy out of the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wutpR-tqCOXX"
      },
      "source": [
        "**To start**, take a look at the **learning rate** for our SGD optimizer on **Line 12** – at $1e-1$ this learning rate is by far the largest we have used in this course (by an order of magnitude). The reason we are able to get away with such a high learning rate is due to the identity mappings built into the residual module. Learning rates this high would not (typically) work for networks such as AlexNet, VGG, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPyPDVvzCjdM"
      },
      "source": [
        "We then **instantiate our ResNet model** on **Line 15**. Here we can see that the network\n",
        "will accept input images with a width of 32 pixels, height of 32 pixels, and depth of 3 (one for each of the RGB channels in the CIFAR-10 dataset). Since the **CIFAR-10 dataset has ten classes**, we’ll learn ten output labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4bLV0vHD7sv"
      },
      "source": [
        "The next parameter we need to supply is (9, 9, 9), or the number of stages in our architecture. This tuple indicates that we will be learning three stages with each stage containing nine residual modules stacked on top of each other. In between each stage, we will apply an additional residual module to decrease the volume size.\n",
        "\n",
        "The next parameter, (64, 64, 128, 256) is the number of filters that the CONV layers will\n",
        "learn. The first CONV layer (before any residual model is applied) will learn K = 64 filters. The remaining entries, 64, 128, and 256 correspond to the number of filters each of the residual module stages will learn. For example, the first nine residual modules will learn K = 64 filters. The second set of nine residual modules will learn K = 128 filters. And finally, the last set of nine residual modules will learn K = 256 filters. The last argument we’ll supply to ResNet is reg, or our L2 regularization strength for weight decay – this value is crucial as it will enable us to prevent\n",
        "overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7tgoWbaID53"
      },
      "source": [
        "# In the case that we need to restart training from a particular epoch\n",
        "start_epoch = 0\n",
        "\n",
        "# Number of epochs you'd like the model to be trained.\n",
        "number_epochs = 10\n",
        "\n",
        "# If the restart training is necessary\n",
        "resume = False\n",
        "\n",
        "if resume == False:\n",
        "  # if there is no specific model checkpoint supplied, then initialize\n",
        "  # the network (ResNet-XX) and compile the model\n",
        "  print(\"[INFO] compiling model...\")\n",
        "\n",
        "  opt = SGD(learning_rate=1e-1)\n",
        "\n",
        "  # def build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
        "  model = ResNet.build(32, 32, 3, 10, (9, 9, 9),(64, 64, 128, 256), reg=0.0005)\n",
        "  model.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=opt,\n",
        "                metrics=[\"accuracy\"])\n",
        "else:\n",
        "\tprint(\"[INFO] loading {}...\".format(\"output/checkpoints/epoch_10.hdf5\"))\n",
        "\tmodel = load_model(\"output/checkpoints/epoch_10.hdf5\")\n",
        "\n",
        "\t# update the learning rate\n",
        "\tprint(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
        "\tK.set_value(model.optimizer.lr, 1e-2)\n",
        "\tprint(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
        "\n",
        "\n",
        "# construct the set of callbacks\n",
        "callbacks = [EpochCheckpoint(\"output/checkpoints\", \n",
        "                             every=5,\n",
        "                             startAt=start_epoch),\n",
        "             TrainingMonitor(\"output/resnetXX_cifar10.png\",\n",
        "                             jsonPath=\"output/resnetXX_cifar10.json\",\n",
        "                             startAt=start_epoch)]\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "model.fit(aug.flow(train_x, train_y, batch_size=128),\n",
        "          validation_data=(test_x, test_y),\n",
        "          steps_per_epoch=len(train_x) // 128, \n",
        "          epochs=number_epochs,\n",
        "          callbacks=callbacks,\n",
        "          verbose=1,\n",
        "          initial_epoch=start_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMJS1Frdyr1G"
      },
      "source": [
        "## 4.3 ResNet on CIFAR-10: Experiment 01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiO5AIP2Fb58"
      },
      "source": [
        "In my very first experiment with CIFAR-10, I was worried about the number of filters in the network, especially regarding overfitting. Because of this concern, my initial filter list consisted of (64, 64, 128, 256) along with (9, 9, 9) stages of residual modules. I also applied a very small amount of L2 regularization with reg=0.0001 – I knew regularization would be needed, but I wasn’t sure on the correct amount (yet). ResNet was trained using SGD with a base learning rate of $1e-1$ and a momentum term of 0.9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COyj4BS9F3Or"
      },
      "source": [
        "# In the case that we need to restart training from a particular epoch\n",
        "start_epoch = 0\n",
        "\n",
        "# Number of epochs you'd like the model to be trained.\n",
        "number_epochs = 50\n",
        "\n",
        "# If the restart training is necessary\n",
        "resume = False\n",
        "\n",
        "if resume == False:\n",
        "  # if there is no specific model checkpoint supplied, then initialize\n",
        "  # the network (ResNet-XX) and compile the model\n",
        "  print(\"[INFO] compiling model...\")\n",
        "\n",
        "  opt = SGD(learning_rate=1e-1)\n",
        "\n",
        "  # def build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
        "  model = ResNet.build(32, 32, 3, 10, (9, 9, 9),(64, 64, 128, 256), reg=0.0005,bnMom=0.9)\n",
        "  model.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=opt,\n",
        "                metrics=[\"accuracy\"])\n",
        "else:\n",
        "\tprint(\"[INFO] loading {}...\".format(\"output/checkpoints/epoch_50.hdf5\"))\n",
        "\tmodel = load_model(\"output/checkpoints/epoch_50.hdf5\")\n",
        "\n",
        "\t# update the learning rate\n",
        "\tprint(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
        "\tK.set_value(model.optimizer.lr, 1e-2)\n",
        "\tprint(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
        "\n",
        "\n",
        "# construct the set of callbacks\n",
        "callbacks = [EpochCheckpoint(\"output/checkpoints\", \n",
        "                             every=5,\n",
        "                             startAt=start_epoch),\n",
        "             TrainingMonitor(\"output/resnetExp01_cifar10.png\",\n",
        "                             jsonPath=\"output/resnetExp01_cifar10.json\",\n",
        "                             startAt=start_epoch)]\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "model.fit(aug.flow(train_x, train_y, batch_size=128),\n",
        "          validation_data=(test_x, test_y),\n",
        "          steps_per_epoch=len(train_x) // 128, \n",
        "          epochs=number_epochs,\n",
        "          callbacks=callbacks,\n",
        "          verbose=1,\n",
        "          initial_epoch=start_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehn_lX-bHiiz"
      },
      "source": [
        "<font color=\"red\">Past epoch 50</font> I noticed training loss starting to slow as well as some volatility in the validation loss (and a growing gap between the two). \n",
        "\n",
        "<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1Nx2NldT_rHWM6fn4glg3nGjxVtelNpUJ\"/>\n",
        "\n",
        "I stopped training, lowered the learning rate to $1e-2$, and then continued training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX8GKTumex27"
      },
      "source": [
        "# In the case that we need to restart training from a particular epoch\n",
        "start_epoch = 50\n",
        "\n",
        "# Number of epochs you'd like the model to be trained.\n",
        "number_epochs = 75\n",
        "\n",
        "# If the restart training is necessary\n",
        "resume = True\n",
        "\n",
        "if resume == False:\n",
        "  # if there is no specific model checkpoint supplied, then initialize\n",
        "  # the network (ResNet-XX) and compile the model\n",
        "  print(\"[INFO] compiling model...\")\n",
        "\n",
        "  opt = SGD(learning_rate=1e-2)\n",
        "\n",
        "  # def build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
        "  model = ResNet.build(32, 32, 3, 10, (9, 9, 9),(64, 64, 128, 256), reg=0.0005,bnMom=0.9)\n",
        "  model.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=opt,\n",
        "                metrics=[\"accuracy\"])\n",
        "else:\n",
        "\tprint(\"[INFO] loading {}...\".format(\"output/checkpoints/epoch_50.hdf5\"))\n",
        "\tmodel = load_model(\"output/checkpoints/epoch_50.hdf5\")\n",
        "\n",
        "\t# update the learning rate\n",
        "\tprint(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
        "\tK.set_value(model.optimizer.lr, 1e-2)\n",
        "\tprint(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
        "\n",
        "\n",
        "# construct the set of callbacks\n",
        "callbacks = [EpochCheckpoint(\"output/checkpoints\", \n",
        "                             every=5,\n",
        "                             startAt=start_epoch),\n",
        "             TrainingMonitor(\"output/resnetExp01_cifar10.png\",\n",
        "                             jsonPath=\"output/resnetExp01_cifar10.json\",\n",
        "                             startAt=start_epoch)]\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "model.fit(aug.flow(train_x, train_y, batch_size=128),\n",
        "          validation_data=(test_x, test_y),\n",
        "          steps_per_epoch=len(train_x) // 128, \n",
        "          epochs=number_epochs,\n",
        "          callbacks=callbacks,\n",
        "          verbose=1,\n",
        "          initial_epoch=start_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOQbq4m5feU1"
      },
      "source": [
        "The drop in learning rate proved very effective, stabilizing validation loss, but also overfitting on the training set start to creep in (in inevitability when working with CIFAR-10) around epoch 75. \n",
        "\n",
        "<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1_BI0gw4tPuAkQOBRqnR2pCCCTuj5XZex\"/>\n",
        "\n",
        "After epoch 75 I once again stopped training, lowered the learning rate to\n",
        "$1e-3$, and allowed ResNet to continue training for another 10 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkJDm6b3om1o"
      },
      "source": [
        "# In the case that we need to restart training from a particular epoch\n",
        "start_epoch = 75\n",
        "\n",
        "# Number of epochs you'd like the model to be trained.\n",
        "number_epochs = 85\n",
        "\n",
        "# If the restart training is necessary\n",
        "resume = True\n",
        "\n",
        "if resume == False:\n",
        "  # if there is no specific model checkpoint supplied, then initialize\n",
        "  # the network (ResNet-XX) and compile the model\n",
        "  print(\"[INFO] compiling model...\")\n",
        "\n",
        "  opt = SGD(learning_rate=1e-3)\n",
        "\n",
        "  # def build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
        "  model = ResNet.build(32, 32, 3, 10, (9, 9, 9),(64, 64, 128, 256), reg=0.0005,bnMom=0.9)\n",
        "  model.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=opt,\n",
        "                metrics=[\"accuracy\"])\n",
        "else:\n",
        "\tprint(\"[INFO] loading {}...\".format(\"output/checkpoints/epoch_75.hdf5\"))\n",
        "\tmodel = load_model(\"output/checkpoints/epoch_75.hdf5\")\n",
        "\n",
        "\t# update the learning rate\n",
        "\tprint(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
        "\tK.set_value(model.optimizer.lr, 1e-3)\n",
        "\tprint(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
        "\n",
        "\n",
        "# construct the set of callbacks\n",
        "callbacks = [EpochCheckpoint(\"output/checkpoints\", \n",
        "                             every=5,\n",
        "                             startAt=start_epoch),\n",
        "             TrainingMonitor(\"output/resnetExp01_cifar10.png\",\n",
        "                             jsonPath=\"output/resnetExp01_cifar10.json\",\n",
        "                             startAt=start_epoch)]\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "model.fit(aug.flow(train_x, train_y, batch_size=128),\n",
        "          validation_data=(test_x, test_y),\n",
        "          steps_per_epoch=len(train_x) // 128, \n",
        "          epochs=number_epochs,\n",
        "          callbacks=callbacks,\n",
        "          verbose=1,\n",
        "          initial_epoch=start_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVN5Er3jpJD-"
      },
      "source": [
        "The final plot is shown below, where we reach 90.40% accuracy on the validation set. For our very first experiment 90.40% is a good start; however, it’s not as high as the\n",
        "90.81% achieved by GoogLeNet in previous lesson. Furthermore, He et al. reported an accuracy of 93% with ResNet on CIFAR-10, so we clearly have some work to do.\n",
        "\n",
        "<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1rUbzGpeMFTWwMhh9RXC7f2i61HjaXdw2\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffk3u5titcUE"
      },
      "source": [
        "## 4.4 Training ResNet on CIFAR-10 with Learning Rate Decay: Experiment 02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUVEjjYo_0cF"
      },
      "source": [
        "At this point, it seems that we have gotten as far as we can using standard ctrl + c training. We’ve also been able to see that our most successful experiments occur when we can train for longer, in the range of 85-100 epochs. However, there are two major problems we need to overcome:\n",
        "\n",
        "1. Whenever we drop the learning rate by an order of magnitude and restart training, we obtain a nice bump in accuracy, but then we quickly plateau.\n",
        "2. We are overfitting.\n",
        "\n",
        "To solve these problems, and boost accuracy further, a good experiment to try is linearly\n",
        "decreasing the learning rate over a large number of epochs, typically about the same as your longest ctrl + c experiments (if not slightly longer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F3f-UjH4JdC"
      },
      "source": [
        "# define the total number of epochs to train for along with the\n",
        "# initial learning rate\n",
        "NUM_EPOCHS = 100\n",
        "INIT_LR = 1e-1\n",
        "\n",
        "def poly_decay(epoch):\n",
        "\t# initialize the maximum number of epochs, base learning rate,\n",
        "\t# and power of the polynomial\n",
        "\tmaxEpochs = NUM_EPOCHS\n",
        "\tbaseLR = INIT_LR\n",
        "\tpower = 1.0\n",
        "\n",
        "\t# compute the new learning rate based on polynomial decay\n",
        "\talpha = baseLR * (1 - (epoch / float(maxEpochs))) ** power\n",
        "\n",
        "\t# return the new learning rate\n",
        "\treturn alpha\n",
        "\n",
        "callbacks = [TrainingMonitor(\"output/resnetExp02_cifar10.png\", jsonPath=\"output/resnetExp02_cifar10.json\"),\n",
        "             LearningRateScheduler(poly_decay)]\n",
        "\n",
        "# initialize the optimizer and model (ResNet-XX)\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = SGD(learning_rate=INIT_LR, momentum=0.9)\n",
        "# def build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
        "model = ResNet.build(32, 32, 3, 10, (9, 9, 9), (64, 64, 128, 256), reg=0.0005)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "model.fit(aug.flow(train_x, train_y, batch_size=128),\n",
        "          validation_data=(test_x, test_y),\n",
        "          steps_per_epoch=len(train_x) // 128, epochs=NUM_EPOCHS,\n",
        "          callbacks=callbacks, verbose=1)\n",
        "\n",
        "# save the network to disk\n",
        "print(\"[INFO] serializing network...\")\n",
        "model.save(\"epoch_100.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp8Mj5q-_h7p"
      },
      "source": [
        "After the 100th epoch, ResNet is reaching 93.55% accuracy on our testing set. This result\n",
        "is substantially higher than our previous experiment, and more importantly, it has allowed us to replicate the results from He et al. when training ResNet on CIFAR-10.\n",
        "Taking a look at the [CIFAR-10 leaderboard](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130), we see that He et al. reached 93.57% accuracy, near identical to our result. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBMAcn-QSCEb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}